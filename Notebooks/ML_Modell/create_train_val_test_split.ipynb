{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data on the disk into a train, validation and test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create for each tile on folder which contains the folder from every season for this tile\n",
    "2. Random sample tiles for the train, validation and test set\n",
    "3. Copy the tiles from the disk to the train, validation and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import rasterio\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIER_REGEX = re.compile(\n",
    "    r\"\"\"(?P<mission>S2[A-B])_MSI\n",
    "        (?P<product_level>L[1-2][A-C])_\n",
    "        (?P<sensing_time>\\d{8}T\\d{6})_\n",
    "        (?P<processing_baseline>N\\d{4})_\n",
    "        (?P<relative_orbit>R\\d{3})_T\n",
    "        (?P<utm_code>\\d{2})\n",
    "        (?P<latitude_band>\\w{1})\n",
    "        (?P<square>\\w{2})_\n",
    "        (?P<year>\\d{4})\n",
    "        (?P<month>\\d{2})\n",
    "        (?P<day>\\d{2})T\n",
    "        (?P<product_time>\\d{6})\"\"\",\n",
    "    re.VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_and_date(identifier: str):\n",
    "    regex_match = re.search(IDENTIFIER_REGEX, identifier)\n",
    "\n",
    "    if not regex_match:\n",
    "        return None, None\n",
    "\n",
    "    utm_code = regex_match.group(\"utm_code\")\n",
    "    latitude_band = regex_match.group(\"latitude_band\")\n",
    "    square = regex_match.group(\"square\")\n",
    "    year = regex_match.group(\"year\")\n",
    "    # remove leading zeros\n",
    "    month = str(int(regex_match.group(\"month\")))\n",
    "    day = str(int(regex_match.group(\"day\")))\n",
    "\n",
    "    tile = f\"{utm_code}{latitude_band}{square}\"\n",
    "    tile_date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "    return tile, tile_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_prepro_path = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\images_only_AOI_test_color_corr_cleaned')\n",
    "masks_prepro_path = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\masks_only_AOI_test_color_corr_cleaned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of all tiles which are available on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_filename(image_dir: Path, set_list: list) -> list:\n",
    "    set_filenames = []\n",
    "    for file_path in image_dir.glob(\"*.pt\"):\n",
    "        tile, number, date = file_path.stem.split(\"_\")\n",
    "        if (tile, number) in set_list:\n",
    "            set_filenames.append(str(file_path))  # .name\n",
    "    return set_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all tiles\n",
    "tile_list = []\n",
    "for file_path in images_prepro_path.glob(\"*.pt\"):\n",
    "    filename = file_path.stem\n",
    "    tile, number, date = filename.split(\"_\")\n",
    "    tile_list.append(tile)\n",
    "\n",
    "# set is used to get unique values of tiles\n",
    "tiles_unique = list(set(tile_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random tiles for train, val and test\n",
    "random.seed(42)\n",
    "random.shuffle(tiles_unique)\n",
    "\n",
    "num_total = len(tiles_unique)\n",
    "num_train = int(num_total * 0.7)\n",
    "num_val = int(num_total * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val and test list\n",
    "train_list = tiles_unique[:num_train]\n",
    "val_list = tiles_unique[num_train : num_train + num_val]\n",
    "test_list = tiles_unique[num_train + num_val :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for each set\n",
    "train_dir = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\data_splitted')\n",
    "for set_name in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(train_dir / set_name, exist_ok=True)\n",
    "    os.makedirs(train_dir / set_name / \"images\", exist_ok=True)\n",
    "    os.makedirs(train_dir / set_name / \"masks\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in images_prepro_path.glob(\"*.pt\"):\n",
    "    file_stammed = file.stem\n",
    "    filename = file.name\n",
    "    tile, _, _ = file_stammed.split(\"_\")\n",
    "    if tile in train_list:\n",
    "        shutil.copyfile(images_prepro_path / filename, train_dir / \"train\" / \"images\" / filename)\n",
    "        shutil.copyfile(masks_prepro_path / filename, train_dir / \"train\" / \"masks\" / filename)\n",
    "    elif tile in val_list:\n",
    "        shutil.copyfile(images_prepro_path / filename, train_dir / \"val\" / \"images\" / filename)\n",
    "        shutil.copyfile(masks_prepro_path / filename, train_dir / \"val\" / \"masks\" / filename)\n",
    "    elif tile in test_list:\n",
    "        shutil.copyfile(images_prepro_path / filename, train_dir / \"test\" / \"images\" / filename)\n",
    "        shutil.copyfile(masks_prepro_path / filename, train_dir / \"test\" / \"masks\" / filename)\n",
    "    else:\n",
    "        print(\"Error: Tile not in any set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in each set:\n",
      "Train images: 3832\n",
      "Train masks: 3832\n",
      "Val images: 489\n",
      "Val masks: 489\n",
      "Test images: 342\n",
      "Test masks: 342\n"
     ]
    }
   ],
   "source": [
    "# check if all files are copied\n",
    "print(\"Number of files in each set:\")\n",
    "print(f'Train images: {len(list(train_dir.glob(\"train/images/*.pt\")))}')\n",
    "print(f'Train masks: {len(list(train_dir.glob(\"train/masks/*.pt\")))}')\n",
    "print(f'Val images: {len(list(train_dir.glob(\"val/images/*.pt\")))}')\n",
    "print(f'Val masks: {len(list(train_dir.glob(\"val/masks/*.pt\")))}')\n",
    "print(f'Test images: {len(list(train_dir.glob(\"test/images/*.pt\")))}')\n",
    "print(f'Test masks: {len(list(train_dir.glob(\"test/masks/*.pt\")))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of all files:\")\n",
    "print(f'Images: {len(list(train_dir.glob(\"**/*.pt\")))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as before, but now with the undersampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally the data needs to be transformed from .tiff to tensor (.pt) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_prepro_path = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\images_undersampling')\n",
    "masks_prepro_path = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\masks_undersampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all tiles\n",
    "tile_list = []\n",
    "for file_path in images_prepro_path.glob(\"*.tif\"):\n",
    "    filename = file_path.stem\n",
    "    tile, number, date = filename.split(\"_\")\n",
    "    tile_list.append(tile)\n",
    "\n",
    "# set is used to get unique values of tiles\n",
    "tiles_unique = list(set(tile_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles: 56\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of tiles: {len(tiles_unique)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random tiles for train, val and test\n",
    "random.seed(42)\n",
    "random.shuffle(tiles_unique)\n",
    "\n",
    "num_total = len(tiles_unique)\n",
    "num_train = int(num_total * 0.7)\n",
    "num_val = int(num_total * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val and test list\n",
    "train_list = tiles_unique[:num_train]\n",
    "val_list = tiles_unique[num_train : num_train + num_val]\n",
    "test_list = tiles_unique[num_train + num_val :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles in each set:\n",
      "Train: 39\n",
      "Val: 11\n",
      "Test: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tiles in each set:\")\n",
    "print(f'Train: {len(train_list)}')\n",
    "print(f'Val: {len(val_list)}')\n",
    "print(f'Test: {len(test_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for each set\n",
    "data_dir = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\data_splitted_undersampling')\n",
    "for set_name in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(data_dir / set_name, exist_ok=True)\n",
    "    os.makedirs(data_dir / set_name / \"images\", exist_ok=True)\n",
    "    os.makedirs(data_dir / set_name / \"masks\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20683 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20683/20683 [09:50<00:00, 35.03it/s]\n"
     ]
    }
   ],
   "source": [
    "num_files = len(list(images_prepro_path.glob(\"*.tif\")))\n",
    "for file in tqdm(images_prepro_path.glob(\"*.tif\"), total=num_files):\n",
    "    file_stammed = file.stem\n",
    "    filename = file.name\n",
    "    tile, _, _ = file_stammed.split(\"_\")\n",
    "\n",
    "    image_path = images_prepro_path / filename\n",
    "    mask_path = masks_prepro_path / filename\n",
    "    \n",
    "    image = rasterio.open(image_path).read()\n",
    "    mask = rasterio.open(mask_path).read()\n",
    "    \n",
    "    image_tensor = torch.from_numpy(image)\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "\n",
    "    if tile in train_list:\n",
    "        torch.save(image_tensor, data_dir / \"train\" / \"images\" / f\"{file_stammed}.pt\")\n",
    "        torch.save(mask_tensor, data_dir / \"train\" / \"masks\" / f\"{file_stammed}.pt\")\n",
    "    elif tile in val_list:\n",
    "        torch.save(image_tensor, data_dir / \"val\" / \"images\" / f\"{file_stammed}.pt\")\n",
    "        torch.save(mask_tensor, data_dir / \"val\" / \"masks\" / f\"{file_stammed}.pt\")\n",
    "    elif tile in test_list:\n",
    "        torch.save(image_tensor, data_dir / \"test\" / \"images\" / f\"{file_stammed}.pt\")\n",
    "        torch.save(mask_tensor, data_dir / \"test\" / \"masks\" / f\"{file_stammed}.pt\")\n",
    "    else:\n",
    "        print(\"Error: Tile not in any set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in each set:\n",
      "Train images: 14178\n",
      "Train masks: 14178\n",
      "Val images: 4648\n",
      "Val masks: 4648\n",
      "Test images: 1857\n",
      "Test masks: 1857\n"
     ]
    }
   ],
   "source": [
    "# check if all files are copied\n",
    "print(\"Number of files in each set:\")\n",
    "print(f'Train images: {len(list(data_dir.glob(\"train/images/*.pt\")))}')\n",
    "print(f'Train masks: {len(list(data_dir.glob(\"train/masks/*.pt\")))}')\n",
    "print(f'Val images: {len(list(data_dir.glob(\"val/images/*.pt\")))}')\n",
    "print(f'Val masks: {len(list(data_dir.glob(\"val/masks/*.pt\")))}')\n",
    "print(f'Test images: {len(list(data_dir.glob(\"test/images/*.pt\")))}')\n",
    "print(f'Test masks: {len(list(data_dir.glob(\"test/masks/*.pt\")))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create geojson file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
