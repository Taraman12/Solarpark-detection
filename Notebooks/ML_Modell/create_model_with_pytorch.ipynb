{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create U-Net with plain PyTorch and timm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a problem with the segmentation-models-pytorch implementation when using it on aws with a cpu only machine, I decided to create my own U-Net with plain PyTorch. I also use the timm library to load a pretrained model as the encoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Load custom dataset\n",
    "2. Create simple feed forward network with dataset\n",
    "3. Add pretrained encoder to network\n",
    "4. Test export to TorchScript (jit)\n",
    "5. Add four input channels to network\n",
    "6. Convert Network to U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: Path, mask_dir:Path, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_files = os.listdir(self.img_dir)\n",
    "        self.mask_files = os.listdir(self.mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.img_dir / self.img_files[idx]\n",
    "        # mask and img_file have so far the same name\n",
    "        mask_path = self.mask_dir / self.img_files[idx]\n",
    "        img = torch.load(img_path)\n",
    "        # converts bool mask into integer (0/1)\n",
    "        mask = torch.load(mask_path).long()\n",
    "        # Apply transform (if any)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, mask #, img_path, mask_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data on the disk into a train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(r'C:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\data_local\\data_splitted')\n",
    "train_img_dir = root / \"train/images\"\n",
    "train_mask_dir = root / \"train/masks\"\n",
    "\n",
    "val_img_dir = root / \"val/images\"\n",
    "val_mask_dir = root / \"val/masks\"\n",
    "\n",
    "test_img_dir = root / \"test/images\"\n",
    "test_mask_dir = root / \"test/masks\"\n",
    "\n",
    "train_dataset = GeoImageDataset(train_img_dir, train_mask_dir)\n",
    "val_dataset = GeoImageDataset(val_img_dir, val_mask_dir)\n",
    "test_dataset = GeoImageDataset(test_img_dir, test_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset ,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=262144, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4*256*256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m----> 9\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import config\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(4, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Module):\n",
    "    def __init__(self, encChannels=(4, 16, 32, 64),\n",
    "            decChannels=(64, 32, 16),\n",
    "            nbClasses=1, retainDim=True,\n",
    "            outSize=(config.INPUT_IMAGE_HEIGHT,  config.INPUT_IMAGE_WIDTH)):\n",
    "        super().__init__()\n",
    "        # initialize the encoder and decoder\n",
    "        self.encoder = Encoder(encChannels)\n",
    "        self.decoder = Decoder(decChannels)\n",
    "        # initialize the regression head and store the class variables\n",
    "        self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "        self.retainDim = retainDim\n",
    "        self.outSize = outSize\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # grab the features from the encoder\n",
    "        encFeatures = self.encoder(x)\n",
    "        # pass the encoder features through decoder making sure that\n",
    "        # their dimensions are suited for concatenation\n",
    "        decFeatures = self.decoder(encFeatures[::-1][0],\n",
    "            encFeatures[::-1][1:])\n",
    "        # pass the decoder features through the regression head to\n",
    "        # obtain the segmentation mask\n",
    "        map = self.head(decFeatures)\n",
    "        # check to see if we are retaining the original output\n",
    "        # dimensions and if so, then resize the output to match them\n",
    "        if self.retainDim:\n",
    "            map = F.interpolate(map, self.outSize)\n",
    "        # return the segmentation map\n",
    "        return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet().to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=config.INIT_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate steps per epoch for training and test set\n",
    "trainSteps = len(train_dataset) // config.BATCH_SIZE\n",
    "testSteps = len(test_dataset) // config.BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:17<05:35, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/20\n",
      "Train loss: 0.595678, Test loss: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:30<04:28, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/20\n",
      "Train loss: 0.595676, Test loss: 0.9483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:43<03:58, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/20\n",
      "Train loss: 0.595679, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:56<03:39, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/20\n",
      "Train loss: 0.595680, Test loss: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:10<03:22, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/20\n",
      "Train loss: 0.595679, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:22<03:05, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 6/20\n",
      "Train loss: 0.595676, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:35<02:51, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 7/20\n",
      "Train loss: 0.595678, Test loss: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:49<02:39, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 8/20\n",
      "Train loss: 0.595676, Test loss: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:02<02:25, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 9/20\n",
      "Train loss: 0.595678, Test loss: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:15<02:11, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 10/20\n",
      "Train loss: 0.595678, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:28<01:57, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 11/20\n",
      "Train loss: 0.595677, Test loss: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:41<01:44, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 12/20\n",
      "Train loss: 0.595677, Test loss: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [02:54<01:32, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 13/20\n",
      "Train loss: 0.595679, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:07<01:18, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 14/20\n",
      "Train loss: 0.595677, Test loss: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:20<01:05, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 15/20\n",
      "Train loss: 0.595676, Test loss: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:33<00:52, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 16/20\n",
      "Train loss: 0.595675, Test loss: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:46<00:39, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 17/20\n",
      "Train loss: 0.595679, Test loss: 0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [04:00<00:26, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 18/20\n",
      "Train loss: 0.595679, Test loss: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [04:13<00:13, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 19/20\n",
      "Train loss: 0.595675, Test loss: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:26<00:00, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 20/20\n",
      "Train loss: 0.595676, Test loss: 0.9481\n",
      "[INFO] total time taken to train the model: 266.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(config.NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(train_dataloader):\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = unet(x)\n",
    "\t\tloss = lossFunc(pred, y.unsqueeze(1).float())\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in test_dataloader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = unet(x)\n",
    "\t\t\ttotalTestLoss += lossFunc(pred, y.unsqueeze(1).float())\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, config.NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(32, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of U-Net from segmentation-models-pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/qubvel/segmentation_models.pytorch/tree/master/segmentation_models_pytorch/decoders/unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/base/modules.py\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=0,\n",
    "        stride=1,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/base/modules.py\n",
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/base/modules.py\n",
    "class Activation(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None or name == \"identity\":\n",
    "            self.activation = nn.Identity(**params)\n",
    "        elif name == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif name == \"softmax2d\":\n",
    "            self.activation = nn.Softmax(dim=1, **params)\n",
    "        elif name == \"softmax\":\n",
    "            self.activation = nn.Softmax(**params)\n",
    "        elif name == \"logsoftmax\":\n",
    "            self.activation = nn.LogSoftmax(**params)\n",
    "        elif name == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif name == \"argmax\":\n",
    "            self.activation = ArgMax(**params)\n",
    "        elif name == \"argmax2d\":\n",
    "            self.activation = ArgMax(dim=1, **params)\n",
    "        elif name == \"clamp\":\n",
    "            self.activation = Clamp(**params)\n",
    "        elif callable(name):\n",
    "            self.activation = name(**params)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh/\"\n",
    "                f\"argmax/argmax2d/clamp/None; got {name}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/base/modules.py\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None:\n",
    "            # no attention\n",
    "            # https://stackoverflow.com/questions/64229717/what-is-the-idea-behind-using-nn-identity-for-residual-learning\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention1 = Attention(attention_type, in_channels=in_channels + skip_channels)\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention2 = Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        n_blocks=5,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if center:\n",
    "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "        blocks = [\n",
    "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, *features: Tuple[Tensor]):\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/base/initialization.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def initialize_decoder(module):\n",
    "    for m in module.modules():\n",
    "\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def initialize_head(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(torch.nn.Module):\n",
    "    def initialize(self):\n",
    "        initialize_decoder(self.decoder)\n",
    "        initialize_head(self.segmentation_head)\n",
    "        if self.classification_head is not None:\n",
    "            initialize_head(self.classification_head)\n",
    "\n",
    "    def check_input_shape(self, x):\n",
    "\n",
    "        h, w = x.shape[-2:]\n",
    "        output_stride = self.encoder.output_stride\n",
    "        if h % output_stride != 0 or w % output_stride != 0:\n",
    "            new_h = (h // output_stride + 1) * output_stride if h % output_stride != 0 else h\n",
    "            new_w = (w // output_stride + 1) * output_stride if w % output_stride != 0 else w\n",
    "            raise RuntimeError(\n",
    "                f\"Wrong input shape height={h}, width={w}. Expected image height and width \"\n",
    "                f\"divisible by {output_stride}. Consider pad your images to shape ({new_h}, {new_w}).\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "\n",
    "        self.check_input_shape(x)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x):\n",
    "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n",
    "\n",
    "        Args:\n",
    "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
    "\n",
    "        Return:\n",
    "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        x = self.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(conv2d, upsampling, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)\n",
    "        flatten = nn.Flatten()\n",
    "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
    "        linear = nn.Linear(in_channels, classes, bias=True)\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(pool, flatten, dropout, linear, activation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_first_conv(model, new_in_channels, default_in_channels=3, pretrained=True):\n",
    "    \"\"\"Change first convolution layer input channels.\n",
    "    In case:\n",
    "        in_channels == 1 or in_channels == 2 -> reuse original weights\n",
    "        in_channels > 3 -> make random kaiming normal initialization\n",
    "    \"\"\"\n",
    "\n",
    "    # get first conv\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) and module.in_channels == default_in_channels:\n",
    "            break\n",
    "\n",
    "    weight = module.weight.detach()\n",
    "    module.in_channels = new_in_channels\n",
    "\n",
    "    if not pretrained:\n",
    "        module.weight = nn.parameter.Parameter(\n",
    "            torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size)\n",
    "        )\n",
    "        module.reset_parameters()\n",
    "\n",
    "    elif new_in_channels == 1:\n",
    "        new_weight = weight.sum(1, keepdim=True)\n",
    "        module.weight = nn.parameter.Parameter(new_weight)\n",
    "\n",
    "    else:\n",
    "        new_weight = torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size)\n",
    "\n",
    "        for i in range(new_in_channels):\n",
    "            new_weight[:, i] = weight[:, i % default_in_channels]\n",
    "\n",
    "        new_weight = new_weight * (default_in_channels / new_in_channels)\n",
    "        module.weight = nn.parameter.Parameter(new_weight)\n",
    "\n",
    "\n",
    "def replace_strides_with_dilation(module, dilation_rate):\n",
    "    \"\"\"Patch Conv2d modules replacing strides with dilation\"\"\"\n",
    "    for mod in module.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            mod.stride = (1, 1)\n",
    "            mod.dilation = (dilation_rate, dilation_rate)\n",
    "            kh, kw = mod.kernel_size\n",
    "            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n",
    "\n",
    "            # Kostyl for EfficientNet\n",
    "            if hasattr(mod, \"static_padding\"):\n",
    "                mod.static_padding = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "\n",
    "class EncoderMixin:\n",
    "    \"\"\"Add encoder functionality such as:\n",
    "    - output channels specification of feature tensors (produced by encoder)\n",
    "    - patching first convolution for arbitrary input channels\n",
    "    \"\"\"\n",
    "\n",
    "    _output_stride = 32\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        \"\"\"Return channels dimensions for each tensor of forward output of encoder\"\"\"\n",
    "        print(self._out_channels[:self._depth + 1])\n",
    "        return self._out_channels[:self._depth + 1]\n",
    "\n",
    "    @property\n",
    "    def output_stride(self):\n",
    "        return min(self._output_stride, 2**self._depth)\n",
    "\n",
    "    def set_in_channels(self, in_channels, pretrained=True):\n",
    "        \"\"\"Change first convolution channels\"\"\"\n",
    "        if in_channels == 3:\n",
    "            return\n",
    "\n",
    "        self._in_channels = in_channels\n",
    "        if self._out_channels[0] == 3:\n",
    "            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n",
    "\n",
    "        patch_first_conv(model=self, new_in_channels=in_channels, pretrained=pretrained)\n",
    "\n",
    "    def get_stages(self):\n",
    "        \"\"\"Override it in your implementation\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_dilated(self, output_stride):\n",
    "\n",
    "        if output_stride == 16:\n",
    "            stage_list = [\n",
    "                5,\n",
    "            ]\n",
    "            dilation_list = [\n",
    "                2,\n",
    "            ]\n",
    "\n",
    "        elif output_stride == 8:\n",
    "            stage_list = [4, 5]\n",
    "            dilation_list = [2, 4]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n",
    "\n",
    "        self._output_stride = output_stride\n",
    "\n",
    "        stages = self.get_stages()\n",
    "        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n",
    "            replace_strides_with_dilation(\n",
    "                module=stages[stage_indx],\n",
    "                dilation_rate=dilation_rate,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n",
    "\n",
    "Attributes:\n",
    "\n",
    "    _out_channels (list of int): specify number of channels for each encoder feature tensor\n",
    "    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n",
    "    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n",
    "\n",
    "Methods:\n",
    "\n",
    "    forward(self, x: torch.Tensor)\n",
    "        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n",
    "        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n",
    "        with resolution same as input `x` tensor).\n",
    "\n",
    "        Input: `x` with shape (1, 3, 64, 64)\n",
    "        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n",
    "                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n",
    "                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n",
    "\n",
    "        also should support number of features according to specified depth, e.g. if depth = 5,\n",
    "        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n",
    "        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(EfficientNet, EncoderMixin):\n",
    "    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "        super().__init__(blocks_args, global_params)\n",
    "\n",
    "        self._stage_idxs = stage_idxs\n",
    "        self._out_channels = out_channels\n",
    "        self._depth = depth\n",
    "        self._in_channels = 3\n",
    "\n",
    "        del self._fc\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [\n",
    "            nn.Identity(),\n",
    "            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "            self._blocks[: self._stage_idxs[0]],\n",
    "            self._blocks[self._stage_idxs[0] : self._stage_idxs[1]],\n",
    "            self._blocks[self._stage_idxs[1] : self._stage_idxs[2]],\n",
    "            self._blocks[self._stage_idxs[2] :],\n",
    "        ]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        stages = self.get_stages()\n",
    "\n",
    "        block_number = 0.0\n",
    "        drop_connect_rate = self._global_params.drop_connect_rate\n",
    "\n",
    "        features = []\n",
    "        for i in range(self._depth + 1):\n",
    "\n",
    "            # Identity and Sequential stages\n",
    "            if i < 2:\n",
    "                x = stages[i](x)\n",
    "\n",
    "            # Block stages need drop_connect rate\n",
    "            else:\n",
    "                for module in stages[i]:\n",
    "                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "                    block_number += 1.0\n",
    "                    x = module(x, drop_connect)\n",
    "\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop(\"_fc.bias\", None)\n",
    "        state_dict.pop(\"_fc.weight\", None)\n",
    "        super().load_state_dict(state_dict, **kwargs)\n",
    "\n",
    "\n",
    "def _get_pretrained_settings(encoder):\n",
    "    pretrained_settings = {\n",
    "        \"imagenet\": {\n",
    "            \"mean\": [0.485, 0.456, 0.406],\n",
    "            \"std\": [0.229, 0.224, 0.225],\n",
    "            \"url\": url_map[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        },\n",
    "        \"advprop\": {\n",
    "            \"mean\": [0.5, 0.5, 0.5],\n",
    "            \"std\": [0.5, 0.5, 0.5],\n",
    "            \"url\": url_map_advprop[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        },\n",
    "    }\n",
    "    return pretrained_settings\n",
    "\n",
    "\n",
    "efficient_net_encoders = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"encoder\": EfficientNetEncoder,\n",
    "        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b0\"),\n",
    "        \"params\": {\n",
    "            \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "            \"stage_idxs\": (3, 5, 9, 16),\n",
    "            \"model_name\": \"efficientnet-b0\",\n",
    "        },\n",
    "    },\n",
    "    \"efficientnet-b1\": {\n",
    "        \"encoder\": EfficientNetEncoder,\n",
    "        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b1\"),\n",
    "        \"params\": {\n",
    "            \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "            \"stage_idxs\": (5, 8, 16, 23),\n",
    "            \"model_name\": \"efficientnet-b1\",\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from timm.models.efficientnet import EfficientNet\n",
    "from timm.models.efficientnet import decode_arch_def, round_channels, default_cfgs\n",
    "from timm.layers.activations import Swish\n",
    "\n",
    "def get_efficientnet_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n",
    "    \"\"\"Create EfficientNet model.\n",
    "    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n",
    "    Paper: https://arxiv.org/abs/1905.11946\n",
    "    EfficientNet params\n",
    "    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n",
    "    'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "    'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "    'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "    'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "    'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "    'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "    'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "    'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "    'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
    "    'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
    "    Args:\n",
    "      channel_multiplier: multiplier to number of channels per layer\n",
    "      depth_multiplier: multiplier to number of repeats per stage\n",
    "    \"\"\"\n",
    "    arch_def = [\n",
    "        [\"ds_r1_k3_s1_e1_c16_se0.25\"],\n",
    "        [\"ir_r2_k3_s2_e6_c24_se0.25\"],\n",
    "        [\"ir_r2_k5_s2_e6_c40_se0.25\"],\n",
    "        [\"ir_r3_k3_s2_e6_c80_se0.25\"],\n",
    "        [\"ir_r3_k5_s1_e6_c112_se0.25\"],\n",
    "        [\"ir_r4_k5_s2_e6_c192_se0.25\"],\n",
    "        [\"ir_r1_k3_s1_e6_c320_se0.25\"],\n",
    "    ]\n",
    "    model_kwargs = dict(\n",
    "        block_args=decode_arch_def(arch_def, depth_multiplier),\n",
    "        num_features=round_channels(1280, channel_multiplier, 8, None),\n",
    "        stem_size=32,\n",
    "        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n",
    "        act_layer=Swish,\n",
    "        drop_rate=drop_rate,\n",
    "        drop_path_rate=0.2,\n",
    "    )\n",
    "    return model_kwargs\n",
    "\n",
    "\n",
    "def gen_efficientnet_lite_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n",
    "    \"\"\"EfficientNet-Lite model.\n",
    "\n",
    "    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n",
    "    Paper: https://arxiv.org/abs/1905.11946\n",
    "\n",
    "    EfficientNet params\n",
    "    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n",
    "      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n",
    "      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n",
    "      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n",
    "      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n",
    "      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),\n",
    "\n",
    "    Args:\n",
    "      channel_multiplier: multiplier to number of channels per layer\n",
    "      depth_multiplier: multiplier to number of repeats per stage\n",
    "    \"\"\"\n",
    "    arch_def = [\n",
    "        [\"ds_r1_k3_s1_e1_c16\"],\n",
    "        [\"ir_r2_k3_s2_e6_c24\"],\n",
    "        [\"ir_r2_k5_s2_e6_c40\"],\n",
    "        [\"ir_r3_k3_s2_e6_c80\"],\n",
    "        [\"ir_r3_k5_s1_e6_c112\"],\n",
    "        [\"ir_r4_k5_s2_e6_c192\"],\n",
    "        [\"ir_r1_k3_s1_e6_c320\"],\n",
    "    ]\n",
    "    model_kwargs = dict(\n",
    "        block_args=decode_arch_def(arch_def, depth_multiplier, fix_first_last=True),\n",
    "        num_features=1280,\n",
    "        stem_size=32,\n",
    "        fix_stem=True,\n",
    "        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n",
    "        act_layer=nn.ReLU6,\n",
    "        drop_rate=drop_rate,\n",
    "        drop_path_rate=0.2,\n",
    "    )\n",
    "    return model_kwargs\n",
    "\n",
    "\n",
    "class EfficientNetBaseEncoder(EfficientNet, EncoderMixin):\n",
    "    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self._stage_idxs = stage_idxs\n",
    "        self._out_channels = out_channels\n",
    "        self._depth = depth\n",
    "        self._in_channels = 3\n",
    "\n",
    "        del self.classifier\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [\n",
    "            nn.Identity(),\n",
    "            nn.Sequential(self.conv_stem, self.bn1),\n",
    "            self.blocks[: self._stage_idxs[0]],\n",
    "            self.blocks[self._stage_idxs[0] : self._stage_idxs[1]],\n",
    "            self.blocks[self._stage_idxs[1] : self._stage_idxs[2]],\n",
    "            self.blocks[self._stage_idxs[2] :],\n",
    "        ]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        stages = self.get_stages()\n",
    "\n",
    "        features = []\n",
    "        for i in range(self._depth + 1):\n",
    "            x = stages[i](x)\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop(\"classifier.bias\", None)\n",
    "        state_dict.pop(\"classifier.weight\", None)\n",
    "        super().load_state_dict(state_dict, **kwargs)\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(EfficientNetBaseEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage_idxs,\n",
    "        out_channels,\n",
    "        depth=5,\n",
    "        channel_multiplier=1.0,\n",
    "        depth_multiplier=1.0,\n",
    "        drop_rate=0.2,\n",
    "    ):\n",
    "        kwargs = get_efficientnet_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n",
    "        super().__init__(stage_idxs, out_channels, depth, **kwargs)\n",
    "\n",
    "\n",
    "class EfficientNetLiteEncoder(EfficientNetBaseEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage_idxs,\n",
    "        out_channels,\n",
    "        depth=5,\n",
    "        channel_multiplier=1.0,\n",
    "        depth_multiplier=1.0,\n",
    "        drop_rate=0.2,\n",
    "    ):\n",
    "        kwargs = gen_efficientnet_lite_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n",
    "        super().__init__(stage_idxs, out_channels, depth, **kwargs)\n",
    "\n",
    "\n",
    "def prepare_settings(settings):\n",
    "    return {\n",
    "        \"mean\": settings.mean,\n",
    "        \"std\": settings.std,\n",
    "        \"url\": settings.url,\n",
    "        \"input_range\": (0, 1),\n",
    "        \"input_space\": \"RGB\",\n",
    "    }\n",
    "\n",
    "\n",
    "timm_efficientnet_encoders = {\n",
    "    \"timm-efficientnet-b0\": {\n",
    "        \"encoder\": EfficientNetEncoder,\n",
    "        \"pretrained_settings\": {\n",
    "            \"imagenet\": prepare_settings(default_cfgs[\"tf_efficientnet_b0\"].cfgs[\"in1k\"]),\n",
    "            \"advprop\": prepare_settings(default_cfgs[\"tf_efficientnet_b0\"].cfgs[\"ap_in1k\"]),\n",
    "            \"noisy-student\": prepare_settings(default_cfgs[\"tf_efficientnet_b0\"].cfgs[\"ns_jft_in1k\"]),\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "            \"stage_idxs\": (2, 3, 5),\n",
    "            \"channel_multiplier\": 1.0,\n",
    "            \"depth_multiplier\": 1.0,\n",
    "            \"drop_rate\": 0.2,\n",
    "        },\n",
    "    },\n",
    "    \"timm-efficientnet-b1\": {\n",
    "        \"encoder\": EfficientNetEncoder,\n",
    "        \"pretrained_settings\": {\n",
    "            \"imagenet\": prepare_settings(default_cfgs[\"tf_efficientnet_b1\"].cfgs[\"in1k\"]),\n",
    "            \"advprop\": prepare_settings(default_cfgs[\"tf_efficientnet_b1\"].cfgs[\"ap_in1k\"]),\n",
    "            \"noisy-student\": prepare_settings(default_cfgs[\"tf_efficientnet_b1\"].cfgs[\"ns_jft_in1k\"]),\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "            \"stage_idxs\": (2, 3, 5),\n",
    "            \"channel_multiplier\": 1.0,\n",
    "            \"depth_multiplier\": 1.1,\n",
    "            \"drop_rate\": 0.2,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "encoders.update(efficient_net_encoders)\n",
    "encoders.update(timm_efficientnet_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import functools\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "def get_encoder(name, in_channels=3, depth=5, weights=None, output_stride=32, **kwargs):\n",
    "\n",
    "    if name.startswith(\"tu-\"):\n",
    "        name = name[3:]\n",
    "        encoder = TimmUniversalEncoder(\n",
    "            name=name,\n",
    "            in_channels=in_channels,\n",
    "            depth=depth,\n",
    "            output_stride=output_stride,\n",
    "            pretrained=weights is not None,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return encoder\n",
    "\n",
    "    try:\n",
    "        Encoder = encoders[name][\"encoder\"]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"Wrong encoder name `{}`, supported encoders: {}\".format(name, list(encoders.keys())))\n",
    "\n",
    "    params = encoders[name][\"params\"]\n",
    "    params.update(depth=depth)\n",
    "    encoder = Encoder(**params)\n",
    "\n",
    "    if weights is not None:\n",
    "        try:\n",
    "            settings = encoders[name][\"pretrained_settings\"][weights]\n",
    "        except KeyError:\n",
    "            raise KeyError(\n",
    "                \"Wrong pretrained weights `{}` for encoder `{}`. Available options are: {}\".format(\n",
    "                    weights,\n",
    "                    name,\n",
    "                    list(encoders[name][\"pretrained_settings\"].keys()),\n",
    "                )\n",
    "            )\n",
    "        encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
    "\n",
    "    encoder.set_in_channels(in_channels, pretrained=weights is not None)\n",
    "    if output_stride != 32:\n",
    "        encoder.make_dilated(output_stride)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n",
    "\n",
    "    if encoder_name.startswith(\"tu-\"):\n",
    "        encoder_name = encoder_name[3:]\n",
    "        if not timm.models.is_model_pretrained(encoder_name):\n",
    "            raise ValueError(f\"{encoder_name} does not have pretrained weights and preprocessing parameters\")\n",
    "        settings = timm.models.get_pretrained_cfg(encoder_name).__dict__\n",
    "    else:\n",
    "        all_settings = encoders[encoder_name][\"pretrained_settings\"]\n",
    "        if pretrained not in all_settings.keys():\n",
    "            raise ValueError(\"Available pretrained options {}\".format(all_settings.keys()))\n",
    "        settings = all_settings[pretrained]\n",
    "\n",
    "    formatted_settings = {}\n",
    "    formatted_settings[\"input_space\"] = settings.get(\"input_space\", \"RGB\")\n",
    "    formatted_settings[\"input_range\"] = list(settings.get(\"input_range\", [0, 1]))\n",
    "    formatted_settings[\"mean\"] = list(settings[\"mean\"])\n",
    "    formatted_settings[\"std\"] = list(settings[\"std\"])\n",
    "\n",
    "    return formatted_settings\n",
    "\n",
    "\n",
    "def get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n",
    "    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n",
    "    return functools.partial(preprocess_input, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "import torch\n",
    "\n",
    "class Unet(SegmentationModel):\n",
    "    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n",
    "    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n",
    "    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n",
    "    for fusing decoder blocks with skip connections.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n",
    "            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n",
    "            Available options are **True, False, \"inplace\"**\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "                **callable** and **None**.\n",
    "            Default is **None**\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 24, 40, 112, 320)\n"
     ]
    }
   ],
   "source": [
    "model = Unet(encoder_name='timm-efficientnet-b0', encoder_weights='imagenet', in_channels=4, activation='sigmoid', classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (encoder): EfficientNetEncoder(\n",
      "    (conv_stem): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNormAct2d(\n",
      "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (drop): Identity()\n",
      "      (act): Swish()\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): DepthwiseSeparableConv(\n",
      "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.013)\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.025)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.038)\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.050)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.062)\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.075)\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.088)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.113)\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.125)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.138)\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.150)\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.163)\n",
      "        )\n",
      "        (3): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.175)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Swish()\n",
      "          )\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): Swish()\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.188)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNormAct2d(\n",
      "      1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (drop): Identity()\n",
      "      (act): Swish()\n",
      "    )\n",
      "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  )\n",
      "  (decoder): UnetDecoder(\n",
      "    (center): Identity()\n",
      "    (blocks): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(296, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (segmentation_head): SegmentationHead(\n",
      "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Activation(\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Tuple, Union\n",
    "def train(\n",
    "    dataloader: Any,\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    # Callable[[torch.Tensor, torch.Tensor], Union[Any, torch.Tensor]]\n",
    "    optimizer: Any,\n",
    ") -> torch.Tensor:\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # ! check why we need to squeeze and convert to float32\n",
    "        loss = loss_fn(pred.squeeze(1), y.to(torch.float32))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "def test(\n",
    "        dataloader: Any,\n",
    "        model: nn.Module,\n",
    "        loss_fn: Any,\n",
    "    ) -> Union[torch.Tensor, torch.Tensor]:\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        metric = BinaryJaccardIndex().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                # test_loss += loss_fn(pred, y).item()\n",
    "                # ToDo: calculate average loss\n",
    "                test_loss += loss_fn(pred.squeeze(1), y.to(torch.float32)).item()\n",
    "                loss = loss_fn(pred.squeeze(1), y.to(torch.float32)).item()\n",
    "\n",
    "                # accuracy\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # ToDo: fix typing\n",
    "        test_loss /= num_batches  # type: ignore\n",
    "        correct /= size  # type: ignore\n",
    "        jaccard_idx = 100 * metric(pred.squeeze(1), y)\n",
    "        print(\n",
    "            f\"Test Error: \\n\"\n",
    "            f\"Jaccard-Index: {(jaccard_idx):>0.3f}%, Avg loss: {test_loss:>5f} \\n\"\n",
    "        )\n",
    "        # ToDo: fix typing\n",
    "        # ToDo: return average loss\n",
    "        return loss, jaccard_idx.item()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 21.920591  [   32/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 23.445992  [  352/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 14.889134  [  672/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 18.747093  [  992/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 26.067984  [ 1312/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 29.890362  [ 1632/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 14.890025  [ 1952/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 22.963402  [ 2272/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 17.288300  [ 2592/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 23.485359  [ 2912/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 19.306440  [ 3232/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "loss: 22.424171  [ 3552/ 3832]\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "UnetDecoder: (<class 'torch.Tensor'>, <class 'torch.Tensor'>)\n",
      "Test Error: \n",
      "Jaccard-Index: 2.124%, Avg loss: 32.569293 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 24, 40, 112, 320)\n",
      "(4, 32, 24, 40, 112, 320)\n",
      "(4, 32, 24, 40, 112, 320)\n",
      "(4, 32, 24, 40, 112, 320)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't redefine method: __out_channels_getter on class: __torch__.___torch_mangle_110.EfficientNetEncoder (of Python compilation unit at: 00000229860A1A10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[1;32m----> 4\u001b[0m model_scripted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(model) \u001b[39m# Export to TorchScript\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_list\u001b[39m(x):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# type: (List[int]) -> List[int]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m   1283\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[0;32m   1285\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[0;32m   1286\u001b[0m     )\n\u001b[0;32m   1288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1289\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 480\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[0;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[0;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[0;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[0;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[0;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[1;32m--> 546\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[0;32m    547\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[1;32mc:\\Users\\Fabian\\Documents\\Github_Masterthesis\\Solarpark-detection\\.venv\\lib\\site-packages\\torch\\jit\\_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    394\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m    395\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 397\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't redefine method: __out_channels_getter on class: __torch__.___torch_mangle_110.EfficientNetEncoder (of Python compilation unit at: 00000229860A1A10)"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "@torch.jit.script\n",
    "def make_list(x):\n",
    "    # type: (List[int]) -> List[int]\n",
    "    return x\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
